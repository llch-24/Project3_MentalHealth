```{r, include = FALSE}
library(tidyverse)
library(janitor)
library(dplyr)
library(reshape2)

```

### Data Reading and Viewing

```{r}
csv <- "analyticdata2024 - clean_data.csv"

mental <- read.csv(csv, header = TRUE)

### Clean data columns
mental %>% clean_names()


### Overview of data as a whole
mental %>% glimpse()
```

### Filtering in relavent correlations between FEATURES Themselves and using a cutoff

```{r}
mental_filtered <- mental %>% select(where(is.numeric))

correlations <- cor(mental_filtered) 

correlations <- melt(correlations, na.rm = TRUE)

cor_feature <- correlations %>% distinct(value, .keep_all = TRUE) %>% filter( (value > 0.70 & value < 1) | value < -0.70)

```

### View Table of relavent correlations between FEATURES:

```{r, include=FALSE}
cor_feature
```

### Filtering in relavent correlations between RESPONSE and Features Themselves and using a cutoff

```{r}

cor_response <- correlations %>% distinct(value, .keep_all = TRUE) %>% filter(Var2 == "Poor.Mental.Health.Days.raw.value")

```

### View Table of relavent correlations between RESPONSE and Feature:

```{r, include=FALSE}
cor_response
```

### Correlation HeatMap For feature matrix:

```{r}
library(ggcorrplot)

cor_matrix <- cor(mental_filtered)

# Filter the correlation matrix to include only the significant correlations
cor_matrix_filtered <- cor_matrix
cor_matrix_filtered[abs(cor_matrix_filtered) < 0.70 | abs(cor_matrix_filtered) >= 1] <- NA

# Visualize the correlation matrix using ggcorrplot
ggcorrplot(cor_matrix_filtered, type = "lower", lab = TRUE)

```

### Model 1

```{r}
mod1 <- lm(Poor.Mental.Health.Days.raw.value ~ (Frequent.Mental.Distress.raw.value * Food.Insecurity.raw.value) + 
             (Food.Insecurity.raw.value * Poor.Physical.Health.Days.raw.value * Poor.or.Fair.Health.raw.value * Physical.Inactivity.raw.value * Frequent.Mental.Distress.raw.value) + (Population.raw.value * Food.Insecurity.numerator)
             
             , data = mental)

mod1 %>% broom::glance()

#mod1$terms

#summary(mod1)
```

```{r}
#car:: vif(mod1)
```

### Model 2

```{r}
mod2 <- lm(Poor.Mental.Health.Days.raw.value ~ (Frequent.Mental.Distress.raw.value * Food.Insecurity.raw.value) + 
             (Food.Insecurity.raw.value * Poor.Physical.Health.Days.raw.value) + (Poor.or.Fair.Health.raw.value * Physical.Inactivity.raw.value) + Frequent.Mental.Distress.raw.value + (Population.raw.value * Food.Insecurity.numerator)
             
             , data = mental)

broom::glance(mod2)
```

```{r}
summary(mod2)
```

```{r}
library(ggfortify)
autoplot(mod2, ncol = 4)
```

### Model 3

```{r}
mod3 <- lm(Poor.Mental.Health.Days.raw.value ~ (Frequent.Mental.Distress.raw.value * Food.Insecurity.raw.value) + 
             (Food.Insecurity.raw.value * Poor.Physical.Health.Days.raw.value) + (Poor.or.Fair.Health.raw.value * Physical.Inactivity.raw.value) + Frequent.Mental.Distress.raw.value + (Population.raw.value * Food.Insecurity.numerator)
             
             , data = mental)

broom::glance(mod3)
```

```{r}
library(ggfortify)
autoplot(mod3, ncol = 4)
```

```{r}
coefplot::coefplot(mod3, intercept = FALSE)
```

### Model 4:

```{r}
mod4 <- lm(Poor.Mental.Health.Days.raw.value ~ (Frequent.Mental.Distress.raw.value * Food.Insecurity.raw.value) + 
             (Food.Insecurity.raw.value * Poor.Physical.Health.Days.raw.value) + (Poor.or.Fair.Health.raw.value * Physical.Inactivity.raw.value) + Frequent.Mental.Distress.raw.value
             
             , data = mental)

broom::glance(mod4)


#summary(mod4)
```

### Model 5

```{r}
mod5 <- lm(Poor.Mental.Health.Days.raw.value ~  Frequent.Mental.Distress.raw.value + Food.Insecurity.raw.value
             
             , data = mental)

mod5 %>% broom::glance()

#mod1$terms

#summary(mod5)
```

### Model 6

```{r}
mod6 <- lm(Poor.Mental.Health.Days.raw.value ~  Frequent.Mental.Distress.raw.value + Adult.Smoking.raw.value + Physical.Inactivity.raw.value + Adult.Obesity.raw.value

             
             , data = mental)

mod6 %>% broom::glance()

#mod1$terms

#summary(mod6)
```

```{r}
summary(mod6)
```

### Model 7 -\> includes high correlated repsonses and New negative correlations

```{r}
### Model 7
mod7 <- lm(Poor.Mental.Health.Days.raw.value ~  Frequent.Mental.Distress.raw.value + Adult.Smoking.raw.value + Physical.Inactivity.raw.value + Adult.Obesity.raw.value + Excessive.Drinking.raw.value + Broadband.Access.raw.value + X..Asian.raw.value + Social.Associations.raw.value



             
             , data = mental)

mod7 %>% broom::glance()

#mod1$terms

#summary(mod6)
```

```{r}
car::vif(mod7)
```

### Model 8 -\> include interactive terms to help further reduce BIC and increase adjusted R\^2

```{r}
### Model 8
mod8 <- lm(Poor.Mental.Health.Days.raw.value ~  Frequent.Mental.Distress.raw.value + Adult.Smoking.raw.value + Physical.Inactivity.raw.value + Adult.Obesity.raw.value + Excessive.Drinking.raw.value + Broadband.Access.raw.value + X..Asian.raw.value + Social.Associations.raw.value + Poor.Physical.Health.Days.raw.value + Poor.or.Fair.Health.raw.value

             , data = mental)

mod8 %>% broom::glance()

```

```{r}
summary(mod8)
```

```{r}
library(ggfortify)
autoplot(mod8, ncol = 4)
```

### Model 9 -\> adding ratio of pop to mental health providers from model 8

```{r}
### Model 9
mod9 <- lm(Poor.Mental.Health.Days.raw.value ~  Frequent.Mental.Distress.raw.value + Adult.Smoking.raw.value + Physical.Inactivity.raw.value + Adult.Obesity.raw.value + Excessive.Drinking.raw.value + Broadband.Access.raw.value + X..Asian.raw.value + Social.Associations.raw.value + Poor.Physical.Health.Days.raw.value + Poor.or.Fair.Health.raw.value + Ratio.of.population.to.mental.health.providers.

             , data = mental)

mod9 %>% broom::glance()

```

### Model 10 -\> All predictors from df -\> mental

```{r}
### Model 10
mod10 <- lm(Poor.Mental.Health.Days.raw.value ~  . , data = mental)

mod10 %>% broom::glance()

```

### Combined Model Evaluation:

```{r}
extract_metrics <- function(mod, mod_name)
{
  broom::glance(mod) %>% mutate(mod_name = mod_name)
}


all_metrics <- purrr::map2_dfr(list(mod1, mod2, mod3, mod4, mod5, mod6, mod7, mod8, mod9, mod10),
                               as.character(1:10),
                               extract_metrics)

all_metrics %>% glimpse()
```

```{r, include = FALSE}
all_metrics %>%
  select(mod_name, df, adj.r.squared, BIC) %>%
  pivot_longer(!c("mod_name")) %>%
  mutate(mod_name = factor(mod_name, levels = 1:10)) %>%
  ggplot(mapping = aes(x = mod_name, y = value)) +
  geom_point(size = 3) +
  facet_wrap(~name, scales = "free_y", ncol = 3) +
  scale_x_discrete(breaks = as.character(1:10)) +
  theme_bw()
```

### Model 10 is absurdly horrible and much more complex compared to the other models so let's get a closer look on the other models, disregarding model 10

```{r}
all_metrics <- purrr::map2_dfr(list(mod1, mod2, mod3, mod4, mod5, mod6, mod7, mod8, mod9),
                               as.character(1:9),
                               extract_metrics)
all_metrics %>% 
  select(mod_name, df, adj.r.squared, AIC, BIC) %>% 
  pivot_longer(!c("mod_name")) %>% 
  ggplot(mapping = aes(x = mod_name, y = value)) +
  geom_point(size = 3) +
  facet_wrap(~name, scales = "free_y") +
  theme_bw()
```

```{r}
all_metrics <- purrr::map2_dfr(list(mod6, mod7, mod8),
                               as.character(6:8),
                               extract_metrics)
all_metrics %>% 
  select(mod_name, df, adj.r.squared, AIC, BIC) %>% 
  pivot_longer(!c("mod_name")) %>% 
  ggplot(mapping = aes(x = mod_name, y = value)) +
  geom_point(size = 3) +
  facet_wrap(~name, scales = "free_y") +
  theme_bw()
```


### Model 7 seems to be the best -> so let's see how it does predicting response

### Let's see how well model 8 does with making predictions...

```{r}
mod_7_pred <- mental |>
  mutate(Mental_Health_Days_predicted_vals = predict(mod7)) |> 
  ggplot(aes(x = Mental_Health_Days_predicted_vals, y = Poor.Mental.Health.Days.raw.value)) +
  geom_point(alpha = 0.2, size = 2, color = 'darkblue') +
  geom_abline(slope = 1, intercept = 0, 
              linetype = "dashed",
              color = "gold2",
              linewidth = 2) +
  labs(y = "Actual Mental Health Days",
       x = "Mental Health Days Predicted Values",
       title = "Model 7 Prediction Plot")
      
  

```

```{r}
mod_8_pred <- mental |>
  mutate(Mental_Health_Days_predicted_vals = predict(mod8)) |> 
  ggplot(aes(x = Mental_Health_Days_predicted_vals, y = Poor.Mental.Health.Days.raw.value)) +
  geom_point(alpha = 0.2, size = 2, color = 'darkblue') +
  geom_abline(slope = 1, intercept = 0, 
              linetype = "dashed",
              color = "gold2",
              linewidth = 2) +
  labs(y = "Actual Mental Health Days",
       x = "Mental Health Days Predicted Values",
       title = "Model 8 Prediction Plot")
      
  

```


# Overfitting?

# Test + Train Split: 5-fold Cross-Validation (80-20):
```{r, include = FALSE}
set.seed(3231)

n_folds <- 5
mental_folds <- mental |>
  mutate(fold_id = sample(rep(1:n_folds, length.out = n())))


mental <- mental |> 
  left_join(mental_folds)

```

### Verify each column got assigned a fold
```{r}
mental %>% count(fold_id)
```

### Model 7
```{r}

models_cv <- function(k) {

  train_data <- mental |> 
    filter(fold_id != k)
  test_data <- mental |> 
    filter(fold_id == k)

  mod_07_fit <- lm(Poor.Mental.Health.Days.raw.value ~  Frequent.Mental.Distress.raw.value + Adult.Smoking.raw.value + Physical.Inactivity.raw.value + Adult.Obesity.raw.value + Excessive.Drinking.raw.value + Broadband.Access.raw.value + X..Asian.raw.value + Social.Associations.raw.value,
                   data = train_data)
  
  mod_07_pred <- predict(mod_07_fit, newdata = test_data)
  
  # return a table with RMSE and fold id
  rmse_result <- tibble(rmse = sqrt(mean((test_data$Poor.Mental.Health.Days.raw.value - mod_07_pred) ^ 2)), 
                        fold_id = k)
  return(rmse_result)
}

mod7_cv_results <- map(1:n_folds, models_cv) |> 
  list_rbind()

```

### Model 8
```{r}

models_cv <- function(k) {

  train_data <- mental |> 
    filter(fold_id != k)
  test_data <- mental |> 
    filter(fold_id == k)

  mod_08_fit <- lm(Poor.Mental.Health.Days.raw.value ~  Frequent.Mental.Distress.raw.value + Adult.Smoking.raw.value + Physical.Inactivity.raw.value + Adult.Obesity.raw.value + Excessive.Drinking.raw.value + Broadband.Access.raw.value + X..Asian.raw.value + Social.Associations.raw.value + Poor.Physical.Health.Days.raw.value + Poor.or.Fair.Health.raw.value,
                   data = train_data)
  
  mod_08_pred <- predict(mod_08_fit, newdata = test_data)
  
  # return a table with RMSE and fold id
  rmse_result <- tibble(rmse = sqrt(mean((test_data$Poor.Mental.Health.Days.raw.value - mod_08_pred) ^ 2)), 
                        fold_id = k)
  return(rmse_result)
}

mod8_cv_results <- map(1:n_folds, models_cv) |> 
  list_rbind()

```

Model 7 and Model 8 RMSE... IS there really a difference?
```{r}
mod7_cv_results |> 
  summarize(avg_rmse = mean(rmse))

mod8_cv_results |> 
  summarize(avg_rmse = mean(rmse))
```

# Model 7 deems to be the best model!!



## Let's see if it overfits or not:

### Train Model 7 Specifically
```{r}
library(caret)

set.seed(3239)
train_control <- trainControl(method = "cv", number = 5) # 5 fold CV

cv_mod_7 <- train(Poor.Mental.Health.Days.raw.value ~  Frequent.Mental.Distress.raw.value + Adult.Smoking.raw.value + Physical.Inactivity.raw.value + Adult.Obesity.raw.value + Excessive.Drinking.raw.value + Broadband.Access.raw.value + X..Asian.raw.value + Social.Associations.raw.value, 
                  data = mental, 
                  method = "lm", 
                  trControl = train_control)


```

### Make predictions
```{r}
mental$predictions <- predict(cv_mod_7, newdata = mental)

mental %>%
  ggplot(aes(x = Poor.Mental.Health.Days.raw.value, y = predictions)) +
  geom_point(alpha = 0.2) +
  geom_abline(slope = 1, intercept = 0, color = "red") +
  ggtitle("Actual vs Predicted Values") +
  xlab("Actual Values") +
  ylab("Predicted Values")

```

### Very well in terms of predictions on the test data with the train data to the model (model 7)

### However, let's assess if model is Overfitting before celebrating...

```{r}
library(caret)

# Split the data into 80% training and 20% test sets
set.seed(3239)
trainIndex <- createDataPartition(mental$Poor.Mental.Health.Days.raw.value, p = 0.8, list = FALSE)
trainData <- mental[trainIndex, ]
testData <- mental[-trainIndex, ]

# Train the model on the training data with 5-fold cross-validation and hyperparameter tuning
train_control <- trainControl(method = "cv", number = 5)

# Define the grid of hyperparameters for tuning
tune_grid <- expand.grid(alpha = 1,  # Lasso regression
                         lambda = seq(0.001, 0.1, by = 0.001))

cv_mod_7 <- train(Poor.Mental.Health.Days.raw.value ~ Frequent.Mental.Distress.raw.value + Adult.Smoking.raw.value + Physical.Inactivity.raw.value + Adult.Obesity.raw.value + Excessive.Drinking.raw.value + Broadband.Access.raw.value + X..Asian.raw.value + Social.Associations.raw.value, 
                  data = trainData, 
                  method = "glmnet", 
                  trControl = train_control, 
                  tuneGrid = tune_grid)

# Make predictions on the training and test data
train_predictions <- predict(cv_mod_7, newdata = trainData)
test_predictions <- predict(cv_mod_7, newdata = testData)

# Evaluate the model on training data
train_r_squared <- cor(trainData$Poor.Mental.Health.Days.raw.value, train_predictions)^2
train_mae <- mean(abs(trainData$Poor.Mental.Health.Days.raw.value - train_predictions))

# Evaluate the model on test data
test_r_squared <- cor(testData$Poor.Mental.Health.Days.raw.value, test_predictions)^2
test_mae <- mean(abs(testData$Poor.Mental.Health.Days.raw.value - test_predictions))

cat("Train R-squared:", train_r_squared, "\n")
cat("Train MAE:", train_mae, "\n")
cat("Test R-squared:", test_r_squared, "\n")
cat("Test MAE:", test_mae, "\n")

# Plot predictions vs actual values for training and test data
ggplot() +
  geom_point(aes(x = trainData$Poor.Mental.Health.Days.raw.value, y = train_predictions, color = "Train"), alpha = 0.75) +
  geom_point(aes(x = testData$Poor.Mental.Health.Days.raw.value, y = test_predictions, color = "Test"), alpha = 0.75) +
  geom_abline(slope = 1, intercept = 0, color = "red") +
  ggtitle("Actual vs Predicted Values") +
  xlab("Actual Values") +
  ylab("Predicted Values") +
  scale_color_manual(name = "Data", values = c("Train" = "blue", "Test" = "orange"))

```

If the training points were significantly closer to the line than the test points, it would suggest overfitting. However, in this case, both sets of points seem to follow the line closely, indicating that the model generalizes well to unseen data.

### Conclusion: Model 7 is the best model to use:

Predictors contained in model 7
```{r}
mod7$coefficients
```
summary/signifigance:
```{r}
summary(mod7)
```





